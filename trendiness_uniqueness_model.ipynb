{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ranking(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return None\n",
    "    text = text.replace(',', '')\n",
    "    match = re.search(r'#([\\d,]+)', text)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "def extract_purchase_count(text):\n",
    "    if pd.isna(text) or not isinstance(text, str) or text.strip() == '':\n",
    "        return 0\n",
    "    match = re.match(r'(\\d+)([Kk]?)\\+', text)\n",
    "    if match:\n",
    "        number = int(match.group(1))\n",
    "        if match.group(2).lower() == 'k':\n",
    "            number *= 1000\n",
    "        return number\n",
    "    return 0\n",
    "\n",
    "def clean_rating(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return None\n",
    "    match = re.match(r'^(\\d+(\\.\\d)?|\\.\\d)$', text.strip())\n",
    "    if match:\n",
    "        return float(text)\n",
    "    return None\n",
    "\n",
    "def clean_review_count(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return None\n",
    "    # Check if the text is a valid integer or a string with commas\n",
    "    if text.isdigit():  # Check if the text is a valid integer\n",
    "        return int(text)\n",
    "    else:  # Handle strings with commas\n",
    "        # Remove commas from the text before converting to integer\n",
    "        text = text.replace(',', '')\n",
    "        try:\n",
    "            return int(text)\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "input_file_path = 'C:\\\\Users\\\\kongj\\\\Desktop\\\\Trendiness Model\\\\amazon_data.csv'\n",
    "output_file_path = 'C:\\\\Users\\\\kongj\\\\Desktop\\\\Trendiness Model\\\\trendiness_data_cleaned.csv'\n",
    "\n",
    "df = pd.read_csv(input_file_path)\n",
    "df['rating'] = df['rating'].apply(clean_rating)\n",
    "df['review_count'] = df['review_count'].apply(clean_review_count)\n",
    "df['rankings'] = df['rankings'].apply(extract_ranking)\n",
    "df['purchase_cnt_prev_month'] = df['purchase_cnt_prev_month'].apply(extract_purchase_count)\n",
    "df.dropna(inplace=True, subset=['review_count', 'rankings', 'rating', 'purchase_cnt_prev_month'])\n",
    "\n",
    "\n",
    "df_cleaned = df[['rating', 'review_count', 'rankings', 'purchase_cnt_prev_month']]\n",
    "df_cleaned.to_csv(output_file_path, index=False)\n",
    "df.to_csv('C:\\\\Users\\\\kongj\\\\Desktop\\\\Trendiness Model\\\\amazon_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv('trendiness_data_cleaned.csv')\n",
    "scaler = MinMaxScaler()\n",
    "data[['purchase_cnt_prev_month', 'review_count', 'rating']] = scaler.fit_transform(data[['purchase_cnt_prev_month', 'review_count', 'rating']])\n",
    "data['rankings'] = 1 - (data['rankings'] / data['rankings'].max())\n",
    "\n",
    "data['trendiness'] = (\n",
    "    0.35 * data['purchase_cnt_prev_month'] +\n",
    "    0.25 * data['rankings'] +\n",
    "    0.2 * data['rating'] +\n",
    "    0.2 * data['review_count']\n",
    ")\n",
    "\n",
    "\n",
    "data['uniqueness'] = (\n",
    "    1 * data['rating'] - \n",
    "    0.4 * data['purchase_cnt_prev_month'] - \n",
    "    0.3 * data['rankings'] - \n",
    "    0.3 * data['review_count']\n",
    ")\n",
    "\n",
    "uniqueness_scaler = MinMaxScaler()\n",
    "data['uniqueness'] = uniqueness_scaler.fit_transform(data[['uniqueness']])\n",
    "\n",
    "data.to_csv('labeled_trendiness_uniqueness_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrendinessUniquenessDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe[['rating', 'review_count', 'rankings', 'purchase_cnt_prev_month']].values\n",
    "        self.trendiness_labels = dataframe['trendiness'].values\n",
    "        self.uniqueness_labels = dataframe['uniqueness'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.tensor(self.data[idx], dtype=torch.float32), \n",
    "                torch.tensor(self.trendiness_labels[idx], dtype=torch.float32),\n",
    "                torch.tensor(self.uniqueness_labels[idx], dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrendinessModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TrendinessModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3_trendiness = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        trendiness = self.sigmoid(self.fc3_trendiness(x))\n",
    "        return trendiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniquenessModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UniquenessModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3_uniqueness = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        uniqueness = self.sigmoid(self.fc3_uniqueness(x))\n",
    "        return uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TrendinessUniquenessDataset(data)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trendiness_model = TrendinessModel()\n",
    "optimizer = optim.Adam(trendiness_model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    trendiness_model.train()\n",
    "    train_loss = 0\n",
    "    for inputs, trendiness_labels, uniqueness_labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        optimizer.zero_grad()\n",
    "        trendiness_outputs = trendiness_model(inputs)\n",
    "        loss_trendiness = criterion(trendiness_outputs.squeeze(), trendiness_labels)\n",
    "        loss_trendiness.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss_trendiness.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    print(f'Epoch {epoch+1}, Training Loss: {train_loss}')\n",
    "\n",
    "with open('trendiness_model.pkl', 'wb') as f:\n",
    "    pickle.dump(trendiness_model.state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueness_model = UniquenessModel()\n",
    "optimizer = optim.Adam(uniqueness_model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    uniqueness_model.train()\n",
    "    train_loss = 0\n",
    "    for inputs, trendiness_labels, uniqueness_labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        optimizer.zero_grad()\n",
    "        uniqueness_outputs = uniqueness_model(inputs)\n",
    "        loss_uniqueness = criterion(uniqueness_outputs.squeeze(), uniqueness_labels)\n",
    "        loss_uniqueness.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss_uniqueness.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    print(f'Epoch {epoch+1}, Training Loss: {train_loss}')\n",
    "\n",
    "with open('uniqueness_model.pkl', 'wb') as f:\n",
    "    pickle.dump(uniqueness_model.state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_trendiness_uniqueness(new_data, trendiness_model, uniqueness_model, scaler):\n",
    "    new_data[['purchase_cnt_prev_month', 'review_count', 'rating']] = scaler.transform(new_data[['purchase_cnt_prev_month', 'review_count', 'rating']])\n",
    "    new_data['rankings'] = 1 - (new_data['rankings'] / new_data['rankings'].max())\n",
    "    inputs = torch.tensor(new_data[['rating', 'review_count', 'rankings', 'purchase_cnt_prev_month']].astype('float32').values)\n",
    "    \n",
    "    trendiness_model.eval()\n",
    "    uniqueness_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        trendiness_outputs = trendiness_model(inputs)\n",
    "        uniqueness_outputs = uniqueness_model(inputs)\n",
    "    \n",
    "    trendiness_scores = trendiness_outputs.squeeze().numpy()\n",
    "    uniqueness_scores = uniqueness_outputs.squeeze().numpy()\n",
    "    \n",
    "    return trendiness_scores, uniqueness_scores\n",
    "\n",
    "loaded_trendiness_model = TrendinessModel()\n",
    "loaded_uniqueness_model = UniquenessModel()\n",
    "\n",
    "with open('trendiness_model.pkl', 'rb') as f:\n",
    "    loaded_trendiness_model.load_state_dict(pickle.load(f))\n",
    "\n",
    "with open('uniqueness_model.pkl', 'rb') as f:\n",
    "    loaded_uniqueness_model.load_state_dict(pickle.load(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_trendiness_model = TrendinessModel()\n",
    "loaded_uniqueness_model = UniquenessModel()\n",
    "\n",
    "with open('trendiness_model.pkl', 'rb') as f:\n",
    "    loaded_trendiness_model.load_state_dict(pickle.load(f))\n",
    "\n",
    "with open('uniqueness_model.pkl', 'rb') as f:\n",
    "    loaded_uniqueness_model.load_state_dict(pickle.load(f))\n",
    "\n",
    "amazon_df = pd.read_csv('amazon_cleaned.csv')\n",
    "\n",
    "trendiness_scores = []\n",
    "uniqueness_scores = []\n",
    "\n",
    "for index, row in amazon_df.iterrows():\n",
    "    new_data_point = row.to_frame().T\n",
    "    trendiness_score, uniqueness_score = predict_trendiness_uniqueness(new_data_point, loaded_trendiness_model, loaded_uniqueness_model, scaler)\n",
    "    trendiness_scores.append(trendiness_score)\n",
    "    uniqueness_scores.append(uniqueness_score)\n",
    "\n",
    "amazon_df['predicted_trendiness'] = trendiness_scores\n",
    "amazon_df['predicted_uniqueness'] = uniqueness_scores\n",
    "\n",
    "amazon_df['predicted_trendiness'] = amazon_df['predicted_trendiness'].astype(float)\n",
    "amazon_df['predicted_uniqueness'] = amazon_df['predicted_uniqueness'].astype(float)\n",
    "\n",
    "amazon_df['trendiness_zscore'] = np.abs(stats.zscore(amazon_df['predicted_trendiness']))\n",
    "amazon_df['uniqueness_zscore'] = np.abs(stats.zscore(amazon_df['predicted_uniqueness']))\n",
    "\n",
    "threshold = 3\n",
    "outliers = amazon_df[(amazon_df['trendiness_zscore'] > threshold) | (amazon_df['uniqueness_zscore'] > threshold)]\n",
    "\n",
    "amazon_df = amazon_df[~((amazon_df['trendiness_zscore'] > threshold) | (amazon_df['uniqueness_zscore'] > threshold))]\n",
    "\n",
    "scaler_trendiness = MinMaxScaler()\n",
    "scaler_uniqueness = MinMaxScaler()\n",
    "\n",
    "amazon_df[['predicted_trendiness']] = scaler_trendiness.fit_transform(amazon_df[['predicted_trendiness']])\n",
    "amazon_df[['predicted_uniqueness']] = scaler_uniqueness.fit_transform(amazon_df[['predicted_uniqueness']])\n",
    "\n",
    "output_file_path = 'C:\\\\Users\\\\kongj\\\\Desktop\\\\Trendiness Model\\\\amazon_predicted_scores.csv'\n",
    "amazon_df.to_csv(output_file_path, index=False)\n",
    "print(f\"Data without outliers and normalized saved to {output_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
